<head>
    <title>wikipediaの記事をpythonへ取り込む方法</title>


    <meta name="keywords" content="python,useful,function,hack">
    {% block meta %}
    <meta name="twitter:card" content="summary_large_image" />
    <!--①-->
    <meta name="twitter:site" content="@matuki_no_ukiwa" />
    <!--②-->
    <meta property="og:url" content="http://www.minekawada.com/wikipedia-python.html" />
    <!--③-->
    <meta property="og:title" content="{{ title }}" />
    <!--④-->
    <meta property="og:description" content="pythonでwikipediaにアクセスして、全知全能になりたい" />
    <!--⑤-->
    <meta property="og:image" content="http://www.minekawada.com/static/Real/wiki/wikipedia_api_link.png" />
    {% endblock %}

</head>




<div id="content">

    <h2>{{ title }}</h2>
    <br>
    <br>

    <h4>目次</h4>
    <ol>
        <li><a href="#install">インストール方法について</a></li><br>

        <li><a href="#candi">pythonでwikipedia検索結果の候補一覧を出す方法</a></li>
        <br>
        <li><a href="#page">pythonでwikipediaのページを出力する方法</a></li>
        <br>
        <li><a href="#page_detail">wikipediaのページをさらに詳しく見ていく</a></li>


    </ol>

    <br>
    <br>
    <hr>


    <p id="install">

    <h2>wikipedia apiのインストール方法</h2>
    <br>

    インストールは以下の一文でOK<br>
    <pre>
                  <code>
pip install wikipedia
                  </code>
              </pre>
    わかりやすくていいですね！


    <br>ここまでできたらサンプルコードを実行してみましょう<br>

    </p>
    <p>

        <br>
        <br>
        <br>

        <hr>

    <h2 id="candi">pythonでwikipedia検索結果の候補一覧を出す方法</h2>
    <pre>
    <code>
import wikipedia


tohomas_result = wikipedia.search("トーマス")

print(tohomas_result)
    </code>
</pre>
    <br>
    <br>
    結果<br>
    <pre>
SyntaxError: Non-ASCII character '\xe3' in file sample.py on line 4, but no encoding declared; see http://python.org/dev/peps/pep-0263/ for details
</pre>
    <br>
    なにやらエラーが出てきました。<br>
    原因はsearchの引数の"トーマス"<br>
    ですので先頭に# coding: UTF-8をつけてもう一度実行<br>
    <h3>pythonで日本語を使う際は、先頭に# coding: UTF-8を入力しましょう！</h3>

    <pre>
    <code>
# coding: UTF-8

import wikipedia


tohomas_result = wikipedia.search("トーマス")

print(tohomas_result)

    </code>
</pre>
    <br>
    結果<br>


    <pre>
<code>
['Perverted Thomas', 'Yuji (actor)', 'List of Thomas & Friends episodes', 'Naomi Watanabe', 'Harukana Receive', 'Brother Tom', 'Dharma & Greg', 'Thomas Kurihara', 'Noguchi', 'Mega Man ZX Advent']
</code>
</pre>

    <br>
    なんか大量に英語で出力が出てきましたね<br>
    日本語版のwikipediaを使いたいのでwikipedia.set_lang("ja")しましょう<br>


    <pre>
<code>

# coding: UTF-8

import wikipedia
wikipedia.set_lang("ja")

tohomas_result = wikipedia.search("トーマス")

print(tohomas_result)

</code>
</pre>
    <br>

    結果<br>
    <pre>
['トマス', 'トーマス郡 (ジョージア州)', 'ゲラント・トーマス', 'デビ・トーマス', 'トーマスタウン 新三郷', 'きかんしゃトーマス トーマスのはじめて物語', '機関車トーマス (絵本)', 'トーマス・カーレンベルグ', 'トーマス・パーテイ', 'トーマス・ハドナー (ミサイル駆逐艦)']
</pre>

    どうやら、wikipediaの検索欄で「トーマス」と検索した時の候補一覧が出てくるようですね！<br>


    <img src="{% static '/Real/wiki/wikipdia_tomas.png' %}">
    </p>
    <p>
        <br>
        <br>
        <br>
        <hr>
        ここでは「機関車トーマス」について調べたいので、候補の「きかんしゃトーマス」をwikipedia.searchの引数に入れましょう<br>



    <h2 id="page">pythonでwikipediaのページを出力する方法</h2>

    wikipediaのページを出力する方法はwikipedia.page()の引数にキーワードを入力するだけです<br>

    <pre>
<code>
# coding: UTF-8

import wikipedia
wikipedia.set_lang("ja")

tohomas_page_result = wikipedia.page("きかんしゃトーマス")

print(tohomas_page_result)
</code>
</pre>

    結果<br>

    <pre>
<code>
    &lt;wikipediaPage 'きかんしゃトーマス'&gt; &lt;/WikipediaPage&gt;
</code>
</pre>
    どうやら、WikipediaPageというオブジェクトが生成されるようです<br>
    おそらくですが、このtohomas_page_resultに「きかんしゃトーマス」と入力した時の結果が詰まっているのでしょう<br>

    <br>
    <br>
    <br>
    <hr>


    </p>


    <p id="page_detail">
    <h2>wikipediaのページをさらに詳しく見ていく</h2>

    ではこの<br>
    &lt;wikipediaPage 'きかんしゃトーマス'&gt; &lt;/WikipediaPage&gt;<br>
    にどんなプロパティが詰まっているのか見ていきましょう！<br>


    <h3>pythonはオブジェクトに対して__dict__を入力すると、そのプロパティ全てが出力されます</h3>
    <br>
    <br>
    コード<br>
    <pre>
    <code>
# coding: UTF-8

import wikipedia

wikipedia.set_lang("ja")
tohomas_page_result = wikipedia.page("きかんしゃトーマス")
print(tohomas_page_result.__dict__)
    </code>
</pre>

    結果
    <pre>
    <code>
{
    'title': 'きかんしゃトーマス', 
    'original_title': 'きかんしゃトーマス', 
    'pageid': '691439',
     'url': 'https://ja.wikipedia.org/wiki/%E3%81%8D%E3%81%8B%E3%82%93%E3%81%97%E3%82%83%E3%83%88%E3%83%BC%E3%83%9E%E3%82%B9'
}
    </code>
</pre>

    このように、プロパティとその値が全て入っています<br>

    <br>
    しかし、これだけだとまだよくわかりませんね...<br>
    大人しくドキュメントを読みますか...<br>


    <a href="https://pypi.org/project/wikipedia/">wikipedia apiのドキュメントリンク</a>


    <img src="{% static '/Real/wiki/wikipedia_api_link.png' %}">


    <br>
    <br>
    <br>
    <hr>

    </p>

</div>




<br>
<br>
<br>
<hr>
</p>

<p id="algo">
<h2>アルゴリズム</h2>
<br>
<br>
今回のアルゴリズムは以下のようにします<br>
<br>

<h4>その１：検索ワードからwikipediaのページを取り出す</h4>
まずは、wikipediaのapiを使って、wikipediaを検索し、ページを取り出します。<br>
具体的なやり方は以下のサイトで、<br>

<a href="wikipedia-python.html">wikipedia-python.html</a>
<br>
ソースコードのみであれば、以下のような形ですね<br>
<pre>
    <code>
# coding: UTF-8

import wikipedia
wikipedia.set_lang("ja")

tohomas_page_result = wikipedia.page("きかんしゃトーマス")

print(tohomas_page_result)
    </code>
</pre>

<br>
<br>
<h4>その２：wikipediaのページのリンクを集める　＆　リンクを配列に追加</h4>
wikipedia.page("検索ワード")によって返される"wikipediaPage オブジェクト"には<br>
"links"というプロパティが含まれています<br>
このプロパティにwikipediaページ内のリンクが全て詰め込まれているのです<br>
このプロパティを利用して、wikipedia内のすべてのリンクを集めます。<br>

<pre>
<code>
# coding: UTF-8

import wikipedia
wikipedia.set_lang("ja")

tohomas_page_result = wikipedia.page("きかんしゃトーマス")

all_links = tohomas_page_result.links
print(all_links)
</code>
</pre>



<br>
<br>
<h4>その３：リンク先に移動し、移動先のリンクを配列に追加</h4>

知りたいのは検索対象のページだけではありません。<br>
リンク先のページも調査対象に含まれます<br>
ですので先ほど集めたリンクすべてに移動して、さらにリンクを集める必要があるのです<br>
<br>
<br>
<br>
<br>
<h4>その４：その２、3 を任意の深さまで繰り返す</h4>
その２とその３を繰り返し行うことで、一番最初に検索したワードを中心に捜査が伸びていきます<br>
これを繰り返すことで、検索したいワードとその周囲に存在する関連ワードを調べることができるのです<br>



<br>
<br>
<br>
<hr>
</p>
<h2 id="source">ソースコード</h2>

<br>
<br>
<br>
注意<br>
その１:<br>
word_listへリンクを収集していきますが、10000以上のリンクが集まったら捜査を打ち切ります<br>

<br>
その２：<br>
再帰処理を行っているので、アルゴリズムは若干複雑です<br>

<br>
その３：<br>
clear_linksで年や月のリンクを削除しています<br>

<br>
その４：<br>
最終的には、リンク(word)とその数(count)のデータを作成しています<br>

<p id="source">
<pre>
    <code>
# coding: UTF-8

import wikipedia
wikipedia.set_lang("ja")

import collections
import pprint

import json
import os


def clear_links(my_list):
    return_list = []
    print(len(my_list))
    for word in my_list:
        if ("月" not in word) and ("年" not in word) :
            return_list.append(word)
    print(len(return_list))
    
    return return_list



def make_list(word):
    word_list = []

    def extend(word, depth = 2):
        if len(word_list) > 10000:
            return
        depth -= 1
        if depth < 0 :
            return
        try:
            next_links = wikipedia.page(word).links
            next_links = clear_links(next_links)
        except:
            return
        print(depth)
        word_list.extend(next_links )
        for word in next_links:
            extend(word, depth)

        #path = "/home/ec2-user/cmysite/static/cloud/data"
    extend(word)


    c = collections.Counter(word_list)
    new_dict = {}
    for key, value in c.items():
        if value <= 1:
            continue
        new_dict[key] = value


    return new_dict


def build_json(my_dict, path):
    #別スレッドを走らせる

    ys = []
    for key, value in my_dict.items():
        data = collections.OrderedDict()
        data["word"] = key
        data["count"] = value
        ys.append( data )

    
    with open(path,'w') as fw:
        json.dump(ys,fw, ensure_ascii=False)


if __name__ == "__main__":
    build_wiki_json("パーシー_(きかんしゃトーマス)")

    
    </code>
</pre>
<br>
<br>
<br>
<hr>
</p>
