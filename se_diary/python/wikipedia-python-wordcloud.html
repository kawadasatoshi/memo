{% extends 'test/base-2/somo.html' %}

{% load static %}
{% block header %}

        <head>
            <title>{{ title }}</title>
            
                    
          <meta name="keywords" content="python,useful,function,hack">
          {% block meta %}
            <meta name="twitter:card" content="summary_large_image" /> <!--①-->
            <meta name="twitter:site" content="@matuki_no_ukiwa" /> <!--②-->
            <meta property="og:url" content="http://www.minekawada.com/wikipedia-python.html" /> <!--③-->
            <meta property="og:title" content="{{ title }}" /> <!--④-->
            <meta property="og:description" content="wikipediaを一目で見れれば、全知全能になれるのでは？" /> <!--⑤-->
            <meta property="og:image" content="http://www.minekawada.com/static/Real/wiki/wikipedia_api_link.png" />
            {% endblock %}

          </head>
          
{% endblock %}



{% block content %}
          
  
        <div id="content">

            <h2>{{ title }}</h2>
          <br>
          <br>

          <h4>目次</h4>
          <ol>
              <li><a href="#pre">前回のあらすじ</a></li><br>

            <li><a href="#algo">アルゴリズム</a></li>
            <br>
            <li><a href="#source">ソースコード</a></li>
            <br>


          </ol>

          <br>
          <br>
          <hr>


          <p id="pre">
              <h2>前回までのあらすじ</h2>

           
<br>
<br>
リンク<br>
              <a href="wikipedia-python.html">wikipedia-python.html</a><br>
              前回はwikipediaへpythonからアクセスする方法を書きました<br>
              しかし、ただwikipediaのページにアクセスしてもおもしろくない<br>
              <br>
              <h4>なので今回はwikipediaのページから、さらに先のページへと連続的にアクセスして、リンクを深掘りしていきたいと思います！</h4>
              最終的な目標は、<br>
              <h4>検索したwikipediaのページを中心にリンクを収集することで、検索ワードを中心にデータを収集し、ワードクラウドを作成する</h4>

<img src="{% static 'wordcloud/wordcloud.png' %}">


<br>
<br>
<br>
<hr>
          </p>

          <p id="algo">
              <h2>アルゴリズム</h2>
              <br>
              <br>
              今回のアルゴリズムは以下のようにします<br>
              <br>

              <h4>その１：検索ワードからwikipediaのページを取り出す</h4>
              まずは、wikipediaのapiを使って、wikipediaを検索し、ページを取り出します。<br>
              具体的なやり方は以下のサイトで、<br>

              <a href="wikipedia-python.html">wikipedia-python.html</a>
              <br>
              ソースコードのみであれば、以下のような形ですね<br>
<pre>
    <code>
# coding: UTF-8

import wikipedia
wikipedia.set_lang("ja")

tohomas_page_result = wikipedia.page("きかんしゃトーマス")

print(tohomas_page_result)
    </code>
</pre>

<br>
              <br>
              <h4>その２：wikipediaのページのリンクを集める　＆　リンクを配列に追加</h4>
              wikipedia.page("検索ワード")によって返される"wikipediaPage オブジェクト"には<br>
              "links"というプロパティが含まれています<br>
              このプロパティにwikipediaページ内のリンクが全て詰め込まれているのです<br>
              このプロパティを利用して、wikipedia内のすべてのリンクを集めます。<br>
              
<pre>
<code>
# coding: UTF-8

import wikipedia
wikipedia.set_lang("ja")

tohomas_page_result = wikipedia.page("きかんしゃトーマス")

all_links = tohomas_page_result.links
print(all_links)
</code>
</pre>



<br>
<br>
              <h4>その３：リンク先に移動し、移動先のリンクを配列に追加</h4>
            
              知りたいのは検索対象のページだけではありません。<br>
              リンク先のページも調査対象に含まれます<br>
ですので先ほど集めたリンクすべてに移動して、さらにリンクを集める必要があるのです<br>
<br>
<br>
              <br>
              <br>
              <h4>その４：その２、3 を任意の深さまで繰り返す</h4>
              その２とその３を繰り返し行うことで、一番最初に検索したワードを中心に捜査が伸びていきます<br>
              これを繰り返すことで、検索したいワードとその周囲に存在する関連ワードを調べることができるのです<br>


              
<br>
<br>
<br>
<hr>
          </p>
<h2 id="source">ソースコード</h2>

<br>
<br>
<br>
注意<br>
その１:<br>
word_listへリンクを収集していきますが、10000以上のリンクが集まったら捜査を打ち切ります<br>

<br>
その２：<br>
再帰処理を行っているので、アルゴリズムは若干複雑です<br>

<br>
その３：<br>
clear_linksで年や月のリンクを削除しています<br>

<br>
その４：<br>
最終的には、リンク(word)とその数(count)のデータを作成しています<br>

<p id="source">
<pre>
    <code>
# coding: UTF-8

import wikipedia
wikipedia.set_lang("ja")

import collections
import pprint

import json
import os


def clear_links(my_list):
    return_list = []
    print(len(my_list))
    for word in my_list:
        if ("月" not in word) and ("年" not in word) :
            return_list.append(word)
    print(len(return_list))
    
    return return_list



def make_list(word):
    word_list = []

    def extend(word, depth = 2):
        if len(word_list) > 10000:
            return
        depth -= 1
        if depth < 0 :
            return
        try:
            next_links = wikipedia.page(word).links
            next_links = clear_links(next_links)
        except:
            return
        print(depth)
        word_list.extend(next_links )
        for word in next_links:
            extend(word, depth)

        #path = "/home/ec2-user/cmysite/static/cloud/data"
    extend(word)


    c = collections.Counter(word_list)
    new_dict = {}
    for key, value in c.items():
        if value <= 1:
            continue
        new_dict[key] = value


    return new_dict


def build_json(my_dict, path):
    #別スレッドを走らせる

    ys = []
    for key, value in my_dict.items():
        data = collections.OrderedDict()
        data["word"] = key
        data["count"] = value
        ys.append( data )

    
    with open(path,'w') as fw:
        json.dump(ys,fw, ensure_ascii=False)


if __name__ == "__main__":
    build_wiki_json("パーシー_(きかんしゃトーマス)")

    
    </code>
</pre>
<br>
<br>
<br>
<hr>
</p>
        </div>


  
  {% endblock %}
  
  
  
  